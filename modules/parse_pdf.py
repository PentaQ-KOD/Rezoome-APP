# resume_processor.py
import re
from datetime import datetime
from langchain_community.document_loaders import PyPDFLoader
from pythainlp.tokenize import word_tokenize
from pythainlp.corpus.common import thai_stopwords
import tempfile
from fixthaipdf import clean
import nltk
from nltk.corpus import stopwords
from modules.embed import get_embedding
from database import MongoDB
from dotenv import load_dotenv
from modules.job_description import call_llama
from together import Together
import json

load_dotenv()
together = Together()

# bring in deps
from llama_cloud_services import LlamaParse
from llama_index.core import SimpleDirectoryReader

nltk.download("stopwords")


class ResumeProcessor:
    def __init__(
        self,
        db_uri="mongodb+srv://inpantawat22:1234@agents.ci2qy.mongodb.net/",
        db_name="db_rezoome",
    ):
        self.db = MongoDB(uri=db_uri, db_name=db_name)
        self.parser = LlamaParse(fast_mode=True)  # ‡πÉ‡∏ä‡πâ fast_mode ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô

    def load_pdf(self, file_path):
        """‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF"""
        try:
            try:
                # ‡πÉ‡∏ä‡πâ LlamaParse ‡∏Å‡πà‡∏≠‡∏ô (‡∏ß‡∏¥‡∏ò‡∏µ‡∏´‡∏•‡∏±‡∏Å)
                documents = SimpleDirectoryReader(
                    input_files=[file_path], file_extractor={".pdf": self.parser}
                ).load_data()

                text = "\n".join([doc.text for doc in documents])
                print(f"üìÑ Loaded and parsed PDF text with LlamaParse")
                return text
            except Exception as e:
                print(f"‚ùå Error using LlamaParse: {e}")
                
                # ‡∏ñ‡πâ‡∏≤ LlamaParse ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ PyPDFLoader (‡∏ß‡∏¥‡∏ò‡∏µ‡∏™‡∏≥‡∏£‡∏≠‡∏á)
                try:
                    from langchain_community.document_loaders import PyPDFLoader
                    print("üìÑ Trying alternative method (PyPDFLoader)...")
                    loader = PyPDFLoader(file_path)
                    documents = loader.load()
                    text = "\n".join([doc.page_content for doc in documents])
                    print("üìÑ Successfully loaded PDF with PyPDFLoader")
                    return text
                except Exception as pdf_error:
                    print(f"‚ùå Error using PyPDFLoader: {pdf_error}")
                    
                    # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡∏≠‡∏µ‡∏Å ‡∏•‡∏≠‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
                    try:
                        import PyPDF2
                        print("üìÑ Trying last resort method (PyPDF2)...")
                        with open(file_path, 'rb') as file:
                            reader = PyPDF2.PdfReader(file)
                            text = ""
                            for page_num in range(len(reader.pages)):
                                text += reader.pages[page_num].extract_text() + "\n"
                        
                        if text.strip():
                            print("üìÑ Successfully loaded PDF with PyPDF2")
                            return text
                        else:
                            print("‚ùå PyPDF2 failed to extract text")
                            return None
                    except Exception as pypdf2_error:
                        print(f"‚ùå Error using PyPDF2: {pypdf2_error}")
                        return None
        except Exception as main_error:
            print(f"‚ùå Main error in load_pdf: {main_error}")
            return None

    def clean_text(self, text):
        # ‡πÉ‡∏ä‡πâ fixthaipdf ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
        cleaned_text = clean(text)

        # ‡πÄ‡∏Å‡πá‡∏ö‡∏≠‡∏µ‡πÄ‡∏°‡∏•‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏ö stopwords ‡πÅ‡∏•‡∏∞ tokenization
        email_matches = list(
            re.finditer(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}", cleaned_text)
        )
        emails = {m.start(): m.group() for m in email_matches}

        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô lowercase ‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô
        # cleaned_text = cleaned_text.lower()
        cleaned_text = " ".join(cleaned_text.split())

        # Tokenization (Thai + English)
        tokens = word_tokenize(cleaned_text, engine="newmm")

        # ‡∏•‡∏ö stopwords (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ + ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)
        stop_words = set(thai_stopwords()).difference({"‡∏ô‡∏≤‡∏¢", "‡∏ô‡∏≤‡∏á", "‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß"})
        english_stopwords = set(stopwords.words("english"))

        tokens = [
            word
            for word in tokens
            if word not in stop_words and word not in english_stopwords
        ]

        # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏á‡∏•‡∏ö stopwords ‡πÅ‡∏•‡∏∞ tokenization
        result_text = " ".join(tokens)

        # ‡πÉ‡∏™‡πà‡∏≠‡∏µ‡πÄ‡∏°‡∏•‡∏Å‡∏•‡∏±‡∏ö‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÄ‡∏î‡∏¥‡∏°
        for pos in sorted(emails.keys(), reverse=True):
            result_text = result_text[:pos] + emails[pos] + result_text[pos:]

        return result_text

    def get_full_resume_text(self, text):
        resume_text = self.load_pdf(text)
        # resume_text = self.clean_text(resume_text)
        return resume_text

    def process_pdf(self, file):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏ó‡∏±‡∏Å‡∏©‡∏∞"""
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
            temp_file.write(file.read())
            temp_path = temp_file.name

        resume_text = self.load_pdf(temp_path)
        print(f"üîç Cleaned Resume Text:\n{resume_text}")
        embedding = get_embedding(resume_text)
        if embedding:
            print(f"‚úÖ Embedding generated successfully:\n{embedding}")
        else:
            print("‚ùå Embedding generation failed.")
            return None, None

        return resume_text, embedding

    def extract_resume_info(self, resume_text):
        """‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà"""
        system_prompt = f"""
        ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Senior Human Resource Specialist ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏á‡∏≤‡∏ô 
        ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏£‡∏´‡∏≤‡∏ö‡∏∏‡∏Ñ‡∏•‡∏≤‡∏Å‡∏£‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô  

        ‡πÇ‡∏õ‡∏£‡∏î‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£:  

        - **name**: ‡∏ä‡∏∑‡πà‡∏≠-‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£  
        - **email**: ‡∏≠‡∏µ‡πÄ‡∏°‡∏•‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠  
        - **phone**: ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå  
        - **address**: ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô  
        - **position**: ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏™‡∏ô‡πÉ‡∏à  
        - **education**: ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ (‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤, ‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô ‡πÅ‡∏•‡∏∞‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤)  
        - **work_experience**: ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á, ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó, ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡∏™‡∏±‡∏á‡πÄ‡∏Ç‡∏õ)  
        - **skills**: ‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (‡∏ó‡∏±‡πâ‡∏á Hard Skills ‡πÅ‡∏•‡∏∞ Soft Skills)  
        - **languages**: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≤‡∏á‡∏†‡∏≤‡∏©‡∏≤ (‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç ‡πÄ‡∏ä‡πà‡∏ô ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô, ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á, ‡∏î‡∏µ, ‡∏î‡∏µ‡∏°‡∏≤‡∏Å)  
        - **certifications**: ‡πÉ‡∏ö‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ô‡∏µ‡∏¢‡∏ö‡∏±‡∏ï‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á  
        - **projects**: ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏ó‡∏≥ (‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£, ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÇ‡∏î‡∏¢‡∏¢‡πà‡∏≠ ‡πÅ‡∏•‡∏∞‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£)  
        - **hobbies**: ‡∏á‡∏≤‡∏ô‡∏≠‡∏î‡∏¥‡πÄ‡∏£‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏á‡∏≤‡∏ô  
        - **references**: ‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á (‡∏ä‡∏∑‡πà‡∏≠, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠)  

        ‡πÇ‡∏õ‡∏£‡∏î‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÅ‡∏•‡∏∞‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏á‡∏≤‡∏ô 
        Output JSON ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô NULL 
        """

        response = together.chat.completions.create(
            model="meta-llama/Llama-3.3-70B-Instruct-Turbo-Free",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": resume_text},
            ],
            temperature=0.3,
        )

        extracted_info = response.choices[0].message.content

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON
        try:
            # ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏•‡∏∞‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï code block ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ
            cleaned_json = re.sub(r"^```json|```$", "", extracted_info.strip())
            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Python dict
            json_data = json.loads(cleaned_json)
            return json_data  # ‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô dict ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô string
        except json.JSONDecodeError as e:
            print(f"‚ùå Error parsing JSON from LLM response: {e}")
            print(f"LLM raw response: {extracted_info}")
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á dict ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
            return {
                "name": None,
                "email": None,
                "phone": None,
                "address": None,
                "education": None,
                "skills": None,
                "work_experience": None,
                "certifications": None,
                "projects": None,
                "hobbies": None,
                "references": None,
                "position": None,
                "languages": None,
            }
