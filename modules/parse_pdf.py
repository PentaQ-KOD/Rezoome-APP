# resume_processor.py
import re
from datetime import datetime
from langchain_community.document_loaders import PyPDFLoader
from pythainlp.tokenize import word_tokenize
from pythainlp.corpus.common import thai_stopwords
import tempfile
from fixthaipdf import clean
import nltk
from nltk.corpus import stopwords
from modules.embed import get_embedding
from utils.database import MongoDB
from dotenv import load_dotenv
from modules.job_description import call_llama
from together import Together
import json

load_dotenv()
together = Together()

# bring in deps
from llama_cloud_services import LlamaParse
from llama_index.core import SimpleDirectoryReader

nltk.download("stopwords")


class ResumeProcessor:
    def __init__(
        self,
        db_uri="mongodb+srv://inpantawat22:1234@agents.ci2qy.mongodb.net/",
        db_name="db_rezoome",
    ):
        self.db = MongoDB(uri=db_uri, db_name=db_name)
        self.parser = LlamaParse(fast_mode=True)  # ‡πÉ‡∏ä‡πâ fast_mode ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô

    def load_pdf(self, file_path):
        """‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF"""
        try:
            try:
                # ‡πÉ‡∏ä‡πâ LlamaParse ‡∏Å‡πà‡∏≠‡∏ô (‡∏ß‡∏¥‡∏ò‡∏µ‡∏´‡∏•‡∏±‡∏Å)
                documents = SimpleDirectoryReader(
                    input_files=[file_path], file_extractor={".pdf": self.parser}
                ).load_data()

                text = "\n".join([doc.text for doc in documents])
                print(f"üìÑ Loaded and parsed PDF text with LlamaParse")
                return text
            except Exception as e:
                print(f"‚ùå Error using LlamaParse: {e}")
                
                # ‡∏ñ‡πâ‡∏≤ LlamaParse ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ PyPDFLoader (‡∏ß‡∏¥‡∏ò‡∏µ‡∏™‡∏≥‡∏£‡∏≠‡∏á)
                try:
                    from langchain_community.document_loaders import PyPDFLoader
                    print("üìÑ Trying alternative method (PyPDFLoader)...")
                    loader = PyPDFLoader(file_path)
                    documents = loader.load()
                    text = "\n".join([doc.page_content for doc in documents])
                    print("üìÑ Successfully loaded PDF with PyPDFLoader")
                    return text
                except Exception as pdf_error:
                    print(f"‚ùå Error using PyPDFLoader: {pdf_error}")
                    return None
                
        except Exception as main_error:
            print(f"‚ùå Main error in load_pdf: {main_error}")
            return None

    def clean_text(self, text):
        # ‡πÉ‡∏ä‡πâ fixthaipdf ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
        cleaned_text = clean(text)

        # ‡πÄ‡∏Å‡πá‡∏ö‡∏≠‡∏µ‡πÄ‡∏°‡∏•‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏ö stopwords ‡πÅ‡∏•‡∏∞ tokenization
        email_matches = list(
            re.finditer(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}", cleaned_text)
        )
        emails = {m.start(): m.group() for m in email_matches}

        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô lowercase ‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô
        # cleaned_text = cleaned_text.lower()
        cleaned_text = " ".join(cleaned_text.split())

        # Tokenization (Thai + English)
        tokens = word_tokenize(cleaned_text, engine="newmm")

        # ‡∏•‡∏ö stopwords (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ + ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)
        stop_words = set(thai_stopwords()).difference({"‡∏ô‡∏≤‡∏¢", "‡∏ô‡∏≤‡∏á", "‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß"})
        english_stopwords = set(stopwords.words("english"))

        tokens = [
            word
            for word in tokens
            if word not in stop_words and word not in english_stopwords
        ]

        # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏á‡∏•‡∏ö stopwords ‡πÅ‡∏•‡∏∞ tokenization
        result_text = " ".join(tokens)

        # ‡πÉ‡∏™‡πà‡∏≠‡∏µ‡πÄ‡∏°‡∏•‡∏Å‡∏•‡∏±‡∏ö‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÄ‡∏î‡∏¥‡∏°
        for pos in sorted(emails.keys(), reverse=True):
            result_text = result_text[:pos] + emails[pos] + result_text[pos:]

        return result_text

    def get_full_resume_text(self, text):
        resume_text = self.load_pdf(text)
        # resume_text = self.clean_text(resume_text)
        return resume_text

    def process_pdf(self, file):
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏ó‡∏±‡∏Å‡∏©‡∏∞"""
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
            temp_file.write(file.read())
            temp_path = temp_file.name

        resume_text = self.load_pdf(temp_path)
        print(f"üîç Cleaned Resume Text:\n{resume_text}")
        embedding = get_embedding(resume_text)
        if embedding:
            print(f"‚úÖ Embedding generated successfully:\n{embedding}")
        else:
            print("‚ùå Embedding generation failed.")
            return None, None

        return resume_text, embedding

    def extract_resume_info(self, resume_text):
        """‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà"""
        # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏Å‡πà‡∏≠‡∏ô
        personal_info_prompt = """
        ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Senior Human Resource Specialist ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏á‡∏≤‡∏ô 
        ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà

        ‡∏Å‡∏é‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:
        1. ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        2. ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ ``` ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô‡πÉ‡∏î‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å JSON
        3. JSON ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ { ‡πÅ‡∏•‡∏∞‡∏à‡∏ö‡∏î‡πâ‡∏ß‡∏¢ } ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        4. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ null
        5. ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

        ‡πÇ‡∏õ‡∏£‡∏î‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:
        1. ‡∏ä‡∏∑‡πà‡∏≠-‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•: ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà
        2. ‡∏≠‡∏µ‡πÄ‡∏°‡∏•: ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏°‡∏µ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö xxx@xxx.xxx
        3. ‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå: ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏°‡∏µ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö 0XX-XXX-XXXX ‡∏´‡∏£‡∏∑‡∏≠ +66XX-XXX-XXXX
        4. ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà: ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠
        5. ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à: ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á Career Objective

        ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á JSON ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£:
        {
            "name": "‡∏ä‡∏∑‡πà‡∏≠-‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•",
            "email": "‡∏≠‡∏µ‡πÄ‡∏°‡∏•",
            "phone": "‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå",
            "address": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà",
            "position": "‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à"
        }
        """

        try:
            # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•
            personal_response = together.chat.completions.create(
                model="meta-llama/Llama-3.3-70B-Instruct-Turbo-Free",
                messages=[
                    {"role": "system", "content": personal_info_prompt},
                    {"role": "user", "content": resume_text},
                ],
                temperature=0.3,
            )
            
            personal_info = personal_response.choices[0].message.content.strip()
            print(f"Personal Info Response: {personal_info}")
            
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î response
            if personal_info.startswith("```json"):
                personal_info = personal_info.replace("```json", "").strip()
            if personal_info.startswith("```"):
                personal_info = personal_info.replace("```", "").strip()
            if personal_info.endswith("```"):
                personal_info = personal_info.replace("```", "").strip()
                
            personal_data = json.loads(personal_info)
            print(f"Parsed Personal Data: {personal_data}")
            
        except Exception as e:
            print(f"Error extracting personal info: {e}")
            personal_data = {
                "name": None,
                "email": None,
                "phone": None,
                "address": None,
                "position": None
            }

        # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏∑‡πà‡∏ô‡πÜ
        other_info_prompt = """
        ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Senior Human Resource Specialist ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏á‡∏≤‡∏ô 
        ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏ã‡∏π‡πÄ‡∏°‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô

        ‡∏Å‡∏é‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:
        1. ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        2. ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ ``` ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô‡πÉ‡∏î‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å JSON
        3. JSON ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ { ‡πÅ‡∏•‡∏∞‡∏à‡∏ö‡∏î‡πâ‡∏ß‡∏¢ } ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        4. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ null
        5. ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

        ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á JSON ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£:
        {
            "education": [
                {
                    "degree": "‡∏ß‡∏∏‡∏í‡∏¥‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤",
                    "institution": "‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤",
                    "year": "‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏ö",
                    "major": "‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤",
                    "gpa": "‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢"
                }
            ],
            "work_experience": [
                {
                    "position": "‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á",
                    "company": "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó",
                    "duration": "‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤",
                    "responsibilities": "‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö"
                }
            ],
            "skills": {
                "technical": ["‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ 1", "‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ 2"],
                "soft": ["‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• 1", "‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• 2"]
            },
            "languages": {
                "‡∏†‡∏≤‡∏©‡∏≤": "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ"
            },
            "certifications": [
                {
                    "name": "‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏ö‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏á",
                    "issuer": "‡∏ú‡∏π‡πâ‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏á",
                    "year": "‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö"
                }
            ],
            "projects": [
                {
                    "name": "‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£",
                    "description": "‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î",
                    "year": "‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏ó‡∏≥"
                }
            ],
            "hobbies": ["‡∏á‡∏≤‡∏ô‡∏≠‡∏î‡∏¥‡πÄ‡∏£‡∏Å 1", "‡∏á‡∏≤‡∏ô‡∏≠‡∏î‡∏¥‡πÄ‡∏£‡∏Å 2"],
            "references": [
                {
                    "name": "‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á",
                    "relationship": "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå",
                    "contact": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠"
                }
            ]
        }
        """

        try:
            # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏∑‡πà‡∏ô‡πÜ
            other_response = together.chat.completions.create(
                model="meta-llama/Llama-3.3-70B-Instruct-Turbo-Free",
                messages=[
                    {"role": "system", "content": other_info_prompt},
                    {"role": "user", "content": resume_text},
                ],
                temperature=0.3,
            )
            
            other_info = other_response.choices[0].message.content.strip()
            print(f"Other Info Response: {other_info}")
            
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î response
            if other_info.startswith("```json"):
                other_info = other_info.replace("```json", "").strip()
            if other_info.startswith("```"):
                other_info = other_info.replace("```", "").strip()
            if other_info.endswith("```"):
                other_info = other_info.replace("```", "").strip()
                
            other_data = json.loads(other_info)
            print(f"Parsed Other Data: {other_data}")
            
        except Exception as e:
            print(f"Error extracting other info: {e}")
            other_data = {
                "education": [],
                "work_experience": [],
                "skills": {"technical": [], "soft": []},
                "languages": {},
                "certifications": [],
                "projects": [],
                "hobbies": [],
                "references": []
            }

        # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        final_data = {**personal_data, **other_data}
        print(f"Final Data: {final_data}")
        
        return final_data
